{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/24BoR/tcfd1HoVcLZd9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Colette-c/MAT-422/blob/main/HW_1_3_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QR Decomposition\n",
        "\n",
        "Using Gram-Schmidt Process to create the set of orthonormal vectors $q_1,...,q_m$ that form a orthonormal basis for the set of vectors $a_1,...,a_m$.\n",
        "\n",
        "We let $A=(a_1,...,a_m)$ and $Q=(q_1,...,q_m)$. The QR decomposition is $A=QR$, where $R$ is a $m \\times m$ matrix that contains the coefficients of the linear combination of $q_i$'s that produces $a_i$. $R$ is an uppertriangular matrix.\n",
        "\n",
        "In the example below, we can see that the product of a matrix $Q$ and a matrix $R$ is the matrix $A$ satisfying $A=QR$"
      ],
      "metadata": {
        "id": "ZAJZSXuLKglI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3QxrNDhKdvh",
        "outputId": "498977ba-bf00-4d70-feb0-f30d41502f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A = \n",
            " [[5 8 3]\n",
            " [4 3 8]\n",
            " [0 5 7]]\n",
            "Q = \n",
            " [[-0.78086881  0.2929674  -0.55173726]\n",
            " [-0.62469505 -0.36620925  0.68967157]\n",
            " [-0.          0.88321055  0.46897667]]\n",
            "R = \n",
            " [[-6.40312424 -8.12103562 -7.34016681]\n",
            " [ 0.          5.66116423  4.13170206]\n",
            " [ 0.          0.          7.1449975 ]]\n",
            "QR = \n",
            " [[5. 8. 3.]\n",
            " [4. 3. 8.]\n",
            " [0. 5. 7.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.random.randint(10, size = (3,3))\n",
        "Q, R = np.linalg.qr(A)\n",
        "print('A = \\n', A)\n",
        "print('Q = \\n', Q)\n",
        "print('R = \\n', R)\n",
        "print('QR = \\n', ((np.matmul(Q,R))))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Least Squares Problems\n",
        "\n",
        "Let $A$ be an $n \\times m$ vector and $b \\in \\mathbb{R}^n$ be a vector. A least-squares problem will try to solve the system $Ax=b$ such that we minimize $||Ax-b||$, where $n>m$. If $A$ is a square matrix, we can just find the matrix inverse. To solve for x, we use QR decomposition, because we know that the solution x will satisfy the equation $Rx=Q^T b$."
      ],
      "metadata": {
        "id": "6xfzHCEKKo5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.random.randint(10, size = (4,3))\n",
        "b = np.random.randint(10, size=(4,1))\n",
        "Q, R = np.linalg.qr(A)\n",
        "x = np.matmul(np.matmul(np.linalg.inv(R),np.transpose(Q)),b)\n",
        "print('b = \\n', b)\n",
        "print('x approximation = \\n',x)\n",
        "print('Ax = \\n', np.around(np.matmul(A,x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CUuVt26Kobo",
        "outputId": "10c33b79-70f1-4b02-9236-2171a40cb627"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b = \n",
            " [[0]\n",
            " [6]\n",
            " [1]\n",
            " [7]]\n",
            "x approximation = \n",
            " [[ 0.25201362]\n",
            " [-0.18964882]\n",
            " [ 0.36889584]]\n",
            "Ax = \n",
            " [[4.]\n",
            " [4.]\n",
            " [1.]\n",
            " [4.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n",
        "Linear Regression is the use of least-squares method to approximate a function to fit a set of data $\\{(x_i,y_i)\\}_{i=1}^n$ where the approx. function can be defined as $\\hat{y}_i = \\beta_0 âˆ‘_{j=1}^d\\beta_jx_{ij}$ where least squares is used to minimize the value of the set of coefficients $\\beta_j$."
      ],
      "metadata": {
        "id": "4BskrPmOKpUD"
      }
    }
  ]
}